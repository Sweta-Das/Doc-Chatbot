from langchain.embeddings import HuggingFaceEmbeddings
from langchain.memory import VectorStoreRetrieverMemory
from langchain.chains import ConversationChain
from langchain_core.prompts import PromptTemplate
from langchain.llms import HuggingFaceEndpoint
from langchain.vectorstores import Chroma
from dotenv import find_dotenv, load_dotenv
import chromadb
import os


def memory_with_VectorStore(query: str):
    load_dotenv(find_dotenv())

    HF_KEY = os.environ['HUGGINGFACE_API_KEY']

    client = chromadb.HttpClient(host="localhost", port=8000)

    # Defining LLM
    llm = HuggingFaceEndpoint(
            repo_id = "mistralai/Mixtral-8x7B-Instruct-v0.1",
            huggingfacehub_api_token = HF_KEY)

    # Query embeddings
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2",
                                    model_kwargs={"device": "cpu"})
        
    # Using vector database as retriever
    vector_db = Chroma(
            collection_name="pdf_embedding_collection",
            embedding_function=embeddings,
            client=client
        )
    
    retriever = vector_db.as_retriever()

    # Memory from vector database
    memory = VectorStoreRetrieverMemory(retriever=retriever)

    # Prompt template
    template = """
    Following is a conversation between a human and an AI. Relevant pieces of previous conversation: {history}
    Don't use any piece of info i.e. not relevant.
    Current conversation:
    Human: {input}
    AI:"""

    # Prompt for LLM
    prompt = PromptTemplate(
        input_variables=["history", "query"],
        template=template
    )

    # Conversation Chain
    conv_with_mem = ConversationChain(
        llm=llm,
        prompt=prompt,
        memory=memory,
        verbose=True # Helps to follow the prompt generated by LLM
    )
    
    res = conv_with_mem.predict(input=query)

    return res